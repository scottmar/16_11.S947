{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Big Data And Society: Lab 3\n",
    "=====\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Importing the library\n",
    "\n",
    "First we need to import the libraries, and some of their components. We will be using **Twython**, a library that provides wrappers around Twitter's API. To install **Twython** on a terminal or the command line, run the following command:\n",
    "```\n",
    "pip install twython\n",
    "```\n",
    "\n",
    "We also need to create a Python file that will contain the **Twitter** keys. It is never a good idea to host these keys on a public website like **github**, so one way to keep them private is importing the keys as a variable from a separate, untracked file. If you haven’t done so, at this point you should obtain your twitter API keys. Create a python file on the same directory of your **IPython** notebook, and name it: **`twitter_key.py`**.\n",
    "```Python\n",
    "# In the file you should define two variables:\n",
    "t_key = ‘your twitter key’\n",
    "t_secret = ‘your twitter secret’\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import twython\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "\n",
    "from datetime import datetime\n",
    "from twython import Twython\n",
    "\n",
    "# Imports the keys from the python file\n",
    "from twitter_key import t_key, t_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('quoE3w9XFxYmKd0R7GPZs0nBB',\n",
       " 'oWVD9xsUVGYHOncmRB0UP3Jz3aSBgu8fbcXubzVx8d1IXq99gm')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_key, t_secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Query the Twitter API\n",
    "Now we are going to construct a **Twython** object; this object simplifies the access to the API, and provides methods for accessing the API’s endpoints. The first function fetches tweets with a given query at a given lat-long. We will be using the search parameters to hit the APIs endpoint. We need to provide the lat/lon of the centroid of the area we want to query, maximum number of tweets to return, and area within the centroid to search for, among others. \n",
    "\n",
    "Additional documentation of the Twython API can be found [here]( https://twython.readthedocs.org/en/latest/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assigns the keys to the variables\n",
    "APP_KEY = t_key\n",
    "APP_SECRET = t_secret\n",
    "\n",
    "''' Fetches tweets with a given query at a given lat-long.'''\n",
    "def get_tweets( latlong=None ):\n",
    "    # Creates a Twithon object with the given keys\n",
    "    twitter = Twython( APP_KEY, APP_SECRET )\n",
    "    # Uses the search function to hit the APIs endpoints and look for recent tweets within the area\n",
    "    results = twitter.search( geocode=','.join([ str(x) for x in latlong ]) + ',15km', result_type='recent', count=10000)\n",
    "    # Returns the only the statuses from the resulting JSON\n",
    "    return results['statuses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latlong = [24.666, 46.7167]\n",
    "print get_tweets(latlong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hit the API and Parse the Result\n",
    "We are going to create a function to help us repeatedly hit the API, and parse the result into a readable JSON that contains the things that we are interested in, and still stores the raw tweet as an additional property. The returned object is a Python `dict` that we can easily parse into another dictionary to later store as a JSON. Raw JSONs returned from the API have a specific structure. It can be sometimes hard to read a raw JSON. I find it easy to use some online parses like [this]( http://json.parser.online.fr/) to look at the structure of the JSON, and only access what we care about. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Does pretty much what its long name suggests. \"\"\"\n",
    "def get_lots_of_tweets( latlong ):\n",
    "    # Create a dictionary to parse the JSON\n",
    "    all_tweets = {}\n",
    "    \n",
    "    # We will be hitting the API a number of times within the total time\n",
    "    total_time = 120\n",
    "    # Everytime we hit the API we subtract time from the total\n",
    "    remaining_seconds = total_time\n",
    "    interval = 30 \n",
    "    while remaining_seconds > 0:\n",
    "        added = 0\n",
    "        # We hit the Twitter API\n",
    "        new_tweets = get_tweets( latlong )\n",
    "        # We parse the resulting JSON, and save the rest of the raw content\n",
    "        for tweet in new_tweets:\n",
    "            tid = tweet['id']\n",
    "            if tid not in all_tweets and tweet['coordinates'] != None:\n",
    "                properties = {}\n",
    "                properties['lat'] = tweet['coordinates']['coordinates'][0]\n",
    "                properties['lon'] = tweet['coordinates']['coordinates'][1]\n",
    "                properties['tweet_id'] = tid\n",
    "                properties['content'] = tweet['text']\n",
    "                properties['user'] = tweet['user']['id']\n",
    "                properties['user_location'] = tweet['user']['location']\n",
    "                properties['raw_source'] = tweet\n",
    "                properties['data_point'] = 'none'\n",
    "                properties['time'] = tweet['created_at']\n",
    "                all_tweets[ tid ] = properties\n",
    "                added += 1\n",
    "        print \"At %d seconds, added %d new tweets, for a total of %d\" % ( total_time - remaining_seconds, added, len( all_tweets ) )\n",
    "        # We wait a few seconds and hit the API again\n",
    "        time.sleep(interval)\n",
    "        remaining_seconds -= interval\n",
    "    # We return the final dictionary\n",
    "    return all_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run the Functions\n",
    "\n",
    "We need to call the functions, and save the JSONs into a location. In this case, I made a folder called `twitter`, where I a m saving all the new JSONS. We can run the code continuously utilizing some loop, or we can use libraries like `threading`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''This function executes the rest of the functions over a given period of time'''\n",
    "def run():\n",
    "    # This is the number of times the code will be executed. In this case, just once. \n",
    "    starting = 1\n",
    "    while starting > 0:\n",
    "        # Sometimes the API returns some errors, killing the whole script, so we setup try/except to make sure it keeps running\n",
    "        try:\n",
    "            # We define a centroid in Riyadh\n",
    "            latlong = [24.6333, 46.7167]\n",
    "            t = get_lots_of_tweets( latlong )\n",
    "            # We name every file with the current time\n",
    "            timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "            # We write a new JSON into the target path\n",
    "            with open( 'twitter\\%stweets.json' %(timestr), 'w' ) as f:\n",
    "                f.write( json.dumps(t))\n",
    "            # we can use a library like threading to execute the run function continuously.\n",
    "            #threading.Timer(10, run).start()\n",
    "            starting -= 1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Parse the JSONs\n",
    "Once we have collected some data, we can parse it, and visualize some of the results. Since some of the data is repeated, we can initialize some lists to check whether or not a tweet already exists, and add it to the list. We can then extract the useful information for our purposes, and store it in another list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import additional libraries\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Get the file names from a given directory\n",
    "file_dir = 'data/sample_tweets'\n",
    "onlyfiles = [ f for f in listdir(file_dir) if isfile(join(file_dir,f)) and not f.startswith('.')]\n",
    "\n",
    "# Initialize some lists to store the points, and the ids of the tweets\n",
    "ids = []\n",
    "all_pts = []\n",
    "# Loop through all the files\n",
    "for file in onlyfiles:\n",
    "    full_dir = join(file_dir,file)\n",
    "    # Open the JSON\n",
    "    with open(full_dir) as f:\n",
    "        data = f.read()\n",
    "        # Load the JSON as a dict\n",
    "        dict = json.loads(data)\n",
    "        # Only add the unique tweets to the list\n",
    "        if not isinstance(dict, list):\n",
    "            for key, val in dict.iteritems():\n",
    "                if key not in ids:\n",
    "                    ids.append(key)\n",
    "                    lat = val['lat']\n",
    "                    lon = val['lon']\n",
    "                    all_pts.append([lat,lon])\n",
    "pts = np.array(all_pts)\n",
    "pts                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Plot some Tweets\n",
    "We can use **matplotlib** to visualize some tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use a scatter plot to make a quick visualization of the data points\n",
    "plt.scatter(pts[:,0], pts[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "Lets make sure that we don't get tweets plotted more than once. How would you make sure to only plot unique tweets? We can maybe:\n",
    "\n",
    "* Construct a list with unique id's\n",
    "* Only add the tweets to the `numpy.array` if the tweet doesn't exist in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
