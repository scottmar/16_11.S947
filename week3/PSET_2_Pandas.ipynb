{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 2: Intro to Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "In this problem set we are going to be working with a processed O-D dataset, Pandas, and Matplotlib.\n",
    "\n",
    "First, a short example will introduce us to the dataset, some data wrangling functions of Pandas, and a few plots with Matplotlib. \n",
    "The OD data was collected based on cell phone towers. Usually, OD datasets are quite big, and contain individual data points, with origin-destination information between cell phone towers. In this case, the dataset has previously been aggregated and processed. While the text files are still very big, their size is significantly smaller than the original dataset. \n",
    "The files that follow the \"Rn_OD_X-Y\" naming structure contain the origin-destination flow from hour X to hour Y with the following columns:\n",
    "\n",
    "* Origin tower ID\n",
    "* Destination tower ID\n",
    "* Closest road intersection to origin \n",
    "* Closest intersection to destination\n",
    "* Origin-destination flow\n",
    "\n",
    "The file \"towers_index.txt\" contains the locations of all the cell towers with the following columns. The row number is the id of the cell tower starting with 1.\n",
    "\n",
    "* Latitude\n",
    "* Longitude\n",
    "* Node ID of the closest intersection to the tower\n",
    "\n",
    "Throughout the exercises we will be continuously using Pandas and some of its merging functionality to join the numerous OD files available. We will be building exploratory data visualizations, finding urban patterns and locating divergent areas within the dataset. Some useful documentation for merging data with Pandas can be found at:\n",
    "\n",
    "\n",
    "http://pandas.pydata.org/pandas-docs/stable/merging.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets import some of our libraries. If you haven't installed seaborn already, please type on your command line:\n",
    "\n",
    "```Python\n",
    "pip install seaborn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# This line lets us plot on our ipython notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s read one of the OD files with Pandas. Since the file has no header, we need to provide the header names. Luckily, we have all the information we need on the `DATA_STRUCTURES.txt` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_01=pd.read_csv(\"data/Hourly Flows/Rn_OD_0-1.txt\", header=None, \n",
    "               names=['oid', 'did', 'close_rid_origin', 'close_rid_dest', 'flow'])\n",
    "df_01.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to import every file with a separate function. \n",
    "\n",
    "`Hint: You can import multiple files with a for loop, or building a function, without having to declare them individually`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_12=pd.read_csv(\"data/Hourly Flows/Rn_OD_1-2.txt\", header=None, \n",
    "               names=['oid', 'did', 'close_rid_origin', 'close_rid_dest', 'flow'])\n",
    "df_12.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you notice, the df structure of both files is similar. We can perform a SQL style join based on a column or index on either of the df’s. In this case, since the rest of the columns are the same, we are only keeping the flow column of the second df.\n",
    "\n",
    "`Hint: You can merge multiple files with a for loop, or building a function, without having to do so individually`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "combined_flow = df_01.join(df_12['flow'], lsuffix='0-1', rsuffix='1-2')\n",
    "combined_flow[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize our newly merged tables with a basic area plot. This plot visualizes the flow within the cell phone towers throughout the 2 different time ranges. By slicing the dataset, we are only plotting some of the towers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_flow[:5][['flow0-1', 'flow1-2']].transpose().plot(kind='area', stacked=False, \n",
    "                                                            legend=True, title='Tower Flows')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Based on the code snippets, and your previous code your job is to build:**\n",
    "\n",
    "* A function that merges the 24 hour ranges into a single DataFrame. The function must have the following inputs/outputs:\n",
    "    1. Inputs: \n",
    "      1. A file path to extract the different files from\n",
    "      * A column name to merge into a single DataFrame\n",
    "      * A list of column names for the DataFrame\n",
    "    2. Outputs: \n",
    "      1. A DataFrame with the merged columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add your code below:\n",
    "#create a list of file names to import\n",
    "filenames = []\n",
    "for x in range(0,24):\n",
    "    string = \"data/Hourly Flows/Rn_OD_\"+str(x)+\"-\"+str(x+1)+\".txt\"\n",
    "    filenames.append(string);\n",
    "    \n",
    "#create a list of dataframes built from the files\n",
    "frames = []\n",
    "for x in range(0,24):\n",
    "    df = pd.read_csv(filenames[x], header=None, names=['oid', 'did', 'close_rid_origin', 'close_rid_dest', 'flow'])\n",
    "    frames.append(df);\n",
    "\n",
    "    \n",
    "#recursively merge all the dataframes, matching on unique oid+did combinations\n",
    "all_flow = df_01\n",
    "for x in range(1,24):\n",
    "    all_flow = pd.merge(all_flow, frames[x], how='outer', on=['oid','did','close_rid_origin','close_rid_dest'],\n",
    "                        suffixes=('','_'+str(x)+str(x+1)));\n",
    "\n",
    "all_flow.rename(columns={'flow': 'flow_01'}, inplace=True)\n",
    "\n",
    "#check to make sure it looks right\n",
    "all_flow[:10]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using all the towers: Make an area plot that shows the relationship between OD flows throughout the 24 hr ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add your code below:\n",
    "#generate the necessary column names\n",
    "col_names=[]\n",
    "for x in range(0,24):\n",
    "    string='flow_'+str(x)+str(x+1)\n",
    "    col_names.append(string);\n",
    "\n",
    "    \n",
    "#make the chart\n",
    "#only the first 10 oid+did combinations are shown; displaying all 477,893 rows would be unwieldy\n",
    "all_flow[:10][col_names].transpose().plot(kind='area', stacked=False, \n",
    "                                                            legend=True, title='Ten Inter-Tower Flows')\n",
    "\n",
    "# if time: fix x-labels to show actual times of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make a dataframe that groups by oid, sums the flows, and sorts by most active towers\n",
    "group_flow = all_flow.groupby('oid').sum()\n",
    "group_flow.reset_index(level=0, inplace=True)\n",
    "group_flow['sum']=group_flow[col_names].sum(axis=1)\n",
    "group_flow_sorted = group_flow.sort_values(by='sum', ascending=False)\n",
    "\n",
    "#now plot the top 10\n",
    "group_flow_sorted[:10][col_names].transpose().plot(kind='area', stacked=False, legend=True,\n",
    "                                                title='Hourly Flows for 10 Most Active OIDs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Choose ONE of the following TWO questions.**\n",
    "* An area plot with **four** different areas:\n",
    "    1. The average flow values across **all** towers\n",
    "    2. The maximum flow values across **all** towers\n",
    "    3. The minimum flow values across **all** towers\n",
    "    4. The median flow values across **all** towers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add your code below:\n",
    "#make a table of summary statistics and clean it up\n",
    "flow_stats = all_flow.describe()\n",
    "flow_stats.drop(['count','std','25%','75%'], inplace=True)\n",
    "flow_stats.drop(['oid','did','close_rid_origin','close_rid_dest'], axis=1, inplace=True)\n",
    "#flow_stats\n",
    "\n",
    "#plot it\n",
    "flow_stats[:][col_names].transpose().plot(kind='area', stacked=False, \n",
    "                                                            legend=True, title='Summary Stats for All Tower Flows')\n",
    "#Can't see detail, so remove the outlier\n",
    "flow_stats.drop('max', inplace=True)\n",
    "\n",
    "flow_stats[:][col_names].transpose().plot(kind='area', stacked=False, \n",
    "                                                            legend=True, title='Zoomed in:')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Repeat the above, but now using the top 10 list generated earlier\n",
    "\n",
    "#make a table of summary statistics and clean it up\n",
    "group_stats = group_flow_sorted[:10].describe()\n",
    "# group_stats = group_flow_sorted.describe()\n",
    "group_stats.drop(['count','std','25%','75%'], inplace=True)\n",
    "group_stats.drop(['oid','did','close_rid_origin','close_rid_dest'], axis=1, inplace=True)\n",
    "#flow_stats\n",
    "\n",
    "\n",
    "#plot it\n",
    "group_stats[:][col_names].transpose().plot(kind='area', stacked=False, \n",
    "                                                            legend=True, title='Summary Stats for 10 Most Active OIDs')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Based on the overall trends that you just noticed with the previous plots, construct:\n",
    "    1. An area plot based on the values of 5 towers with flow values that are closer to the average trend.\n",
    "    2. An area plot based on the outlier values. You will plot 3 towers with flow values that are closer to the minimum trend, and towers with flow values that are closer to the maximum trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add your code below:\n",
    "# Chose the other question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Now we will use the additional file provided to display the geographical location of every tower. We will first import the file and look at its structure. Since the file has no header, we will use the information provided on the `DATA_STRUCTURES.txt` file. The row number corresponds to the tower index; in order to relate every tower with the OD files, we will add an additional column with the row number as an index. We can do so with the following `DataFrame` function:\n",
    "```Python\n",
    "DataFrame.reset_index(level=0, inplace=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "towers=pd.read_csv(\"data/Hourly Flows/towers_index.txt\", header=None, \n",
    "               names=['Latitude', 'Longitude', 'Closest node ID'])\n",
    "# Use the row value as the index column\n",
    "towers.reset_index(level=0, inplace=True)\n",
    "towers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the structure of the OD file, every tower is repeated several times: for every origin tower, there are several destinations. In SQL terms, we can use a `one-to-many` relationship to assign the geographic values of the towers, to the origin or destination towers. By joining a `DataFrame` `on` a given column, Pandas will assign the row value of every element that matches the value of the given column on the other `DataFrame`. We will also group the origin values of the OD Dataframe, and get their average before joining it with the towers’ geographical location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from the OD df, use only two columns, then group them by 'oid'\n",
    "grouped_01 = df_01[['flow','oid']].groupby('oid').mean()\n",
    "\n",
    "# use the oid as a column, not the index\n",
    "grouped_01.reset_index(level=0, inplace=True)\n",
    "\n",
    "# join with the tower df based on the oid\n",
    "OD_location = grouped_01.join(towers, on='oid')\n",
    "OD_location.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a new `DataFrame` with an average flow value for every origin tower, and their corresponding geographical location, we can use their lat/lon values to create a scatter plot. We can even vary their size based on the average flow. Since the flow variation across tower is not that big, we can use a log function to better visualize it on our plot. Numpy has a quick log implementation:\n",
    "```Python\n",
    "np.log(value)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(OD_location['Longitude'],OD_location['Latitude'], s=np.log(OD_location['flow'])*100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Based on the code snippets, and your previous code your job is to build:**\n",
    "\n",
    "* A function that creates a [color range](http://matplotlib.org/users/colormaps.html) based on a `numpy array` of numeric values [(Example 1)](http://stackoverflow.com/questions/12965075/matplotlib-scatter-plot-colour-as-function-of-third-variable), [(Example 2)](http://stackoverflow.com/questions/11364745/how-can-i-turn-a-numpy-array-into-a-matplotlib-colormap). The function must have the following inputs/outputs:\n",
    "    1. Inputs: \n",
    "      1. A `numpy` `array` or a `Pandas DataSeries`\n",
    "      * Two different colors to interpolate values between.\n",
    "    2. Outputs: \n",
    "      1. A `numpy array` of `colors` or `color values` \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add your code below:\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# make a function that transforms two colors into a colormap\n",
    "\n",
    "def make_cmap(ColorP, ColorQ):\n",
    "    cmdict = {'red':  [(0.0,  0.0, 0.0),\n",
    "                       (1.0,  0.0, 0.0)],\n",
    "\n",
    "             'green': [(0.0,  0.0, 0.0),\n",
    "                       (1.0,  0.0, 0.0)],\n",
    "\n",
    "             'blue':  [(0.0,  0.0, 0.0),\n",
    "                        (1.0,  0.0, 0.0)]}\n",
    "\n",
    "#Convert input colors to rgb and feed the values into the color matrix\n",
    "    colorA = colors.colorConverter.to_rgb(ColorP)\n",
    "    colorB = colors.colorConverter.to_rgb(ColorQ)\n",
    "\n",
    "    cmdict['red'] = [(0.0, colorA[0], colorA[0]),\n",
    "                     (1.0, colorB[0], colorB[0])]\n",
    "    cmdict['green'] = [(0.0, colorA[1], colorA[1]),\n",
    "                       (1.0, colorB[1], colorB[1])]\n",
    "    cmdict['blue'] = [(0.0, colorA[2], colorA[2]),\n",
    "                      (1.0, colorB[2], colorB[2])]\n",
    "\n",
    "    my_cmap = LinearSegmentedColormap('MyCMAP', cmdict)\n",
    "    return my_cmap;\n",
    "\n",
    "# try it on a sample dataframe\n",
    "x = OD_location['Longitude']\n",
    "y = OD_location['Latitude']\n",
    "z = OD_location['index']\n",
    "\n",
    "\n",
    "plt.scatter(x, y, marker='+', c=z, cmap=make_cmap('aqua','orange'), s=150, linewidths=1)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# The following function, written to produce a list of colors from the colormap, doesn't work.\n",
    "# However it is easy enough to pass both a colormap and an array into scatter()\n",
    "# so that is how I generated the later scatterplots.\n",
    "\n",
    "# def pick_colors(colorP, colorQ, z):\n",
    "#     cmtemp = make_cmap(colorP, colorQ)\n",
    "#     cmtemp2 = plt.cm.get_cmap(cmtemp)\n",
    "#     z_array = np.array(z)\n",
    "#     z_norm = plt.Normalize(vmin=0,vmax=1)\n",
    "#     color_list = []\n",
    "#     for item in z_array:\n",
    "#         colortemp = cmtemp2((item-z_array.min())/z_array.ptp())\n",
    "#         color_list.append(colortemp);\n",
    "#     return color_list;\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the plots below should be scatter plots. Parameterize the size of the points to the obtained values.\n",
    "\n",
    "* A plot that visualizes the average flow values using the 24 hour ranges. Color the plots using the function to create the color array.\n",
    "    * The `Dataframe` must be grouped and averaged by 'oid'\n",
    "    * The flow values must be averaged across the 24 hr ranges. IE\n",
    "    ```Python\n",
    "    flow_ranges = [5, 5, 5, 5, 5, 5, 10, 10, 10, 10, 10, 10]\n",
    "    avg_flow = 7.5\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add your code below:\n",
    "\n",
    "#add a column to average across the timeframe\n",
    "all_flow['avg_flow'] = all_flow[col_names].mean(axis=1)\n",
    "\n",
    "# group them on OID, averaging values within each\n",
    "grouped_avg = all_flow[['oid','avg_flow']].groupby('oid').mean()\n",
    "grouped_avg.reset_index(level=0, inplace=True)\n",
    "\n",
    "# merge it with the towers data, first forcing tower id to start at \"1\" so it aligns properly\n",
    "towers['oid'] = towers['index']+1\n",
    "flow_loc_avg = pd.merge(grouped_avg, towers, how='outer', on='oid', suffixes=('',''))\n",
    "\n",
    "# define z and use the plotting function\n",
    "z_avg = flow_loc_avg['avg_flow']\n",
    "\n",
    "plt.scatter(flow_loc_avg['Longitude'], flow_loc_avg['Latitude'], np.log(z_avg)*100,\n",
    "           c=z_avg, cmap=make_cmap('white','red'))\n",
    "plt.ylabel('Latitude')\n",
    "plt.xlabel('Longitude')\n",
    "plt.title('Average Hourly Flows by Location')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A plot that visualizes the maximum flow values using the 24 hour ranges. Color the plots using the function to create the color array.\n",
    "    * The `Dataframe` must be grouped by 'oid', you can then use these values to find the `max()`\n",
    "    * Use those flow values to find the `max()` across the 24 hr ranges. IE\n",
    "    ```Python\n",
    "    flow_ranges = [5, 5, 5, 5, 5, 5, 10, 10, 10, 10, 10, 100]\n",
    "    max_flow = 100\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add your code below:\n",
    "#add the max flow column\n",
    "all_flow['max_flow'] = all_flow[col_names].max(axis=1)\n",
    "\n",
    "# group on oid, averaging the maxes within each\n",
    "grouped_max = all_flow[['oid','max_flow']].groupby('oid').mean()\n",
    "grouped_max.reset_index(level=0, inplace=True)\n",
    "\n",
    "#merge it with the towers data\n",
    "flow_loc_max = pd.merge(grouped_max, towers, how='outer', on='oid', suffixes=('',''))\n",
    "\n",
    "#define z and use the plotting function\n",
    "z_max = flow_loc_max['max_flow']\n",
    "\n",
    "plt.scatter(flow_loc_max['Longitude'], flow_loc_max['Latitude'], np.log(z_max)*100,\n",
    "           c=z_max, cmap=make_cmap('white','blue'))\n",
    "plt.ylabel('Latitude')\n",
    "plt.xlabel('Longitude')\n",
    "plt.title('Maximum Daily Flows by Location')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A plot that visualizes the minimum flow values using the 24 hour ranges. Color the plots using the function to create the color array.\n",
    "    * The `Dataframe` must be grouped by 'oid', you can then use these values to find the `min()`\n",
    "    * Use those flow values to find the `min()` across the 24 hr ranges. IE\n",
    "    ```Python\n",
    "    flow_ranges = [5, 5, 5, 5, 5, 5, 10, 10, 10, 10, 10, 100]\n",
    "    min_flow = 5\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add your code below:\n",
    "#add the min flow column\n",
    "all_flow['min_flow'] = all_flow[col_names].min(axis=1)\n",
    "\n",
    "# group on oid, averaging the mins within each\n",
    "grouped_min = all_flow[['oid','min_flow']].groupby('oid').mean()\n",
    "grouped_min.reset_index(level=0, inplace=True)\n",
    "\n",
    "#merge it with the towers data\n",
    "flow_loc_min = pd.merge(grouped_min, towers, how='outer', on='oid', suffixes=('',''))\n",
    "\n",
    "#define z and use the plotting function\n",
    "z_min = flow_loc_min['min_flow']\n",
    "\n",
    "plt.scatter(flow_loc_min['Longitude'], flow_loc_min['Latitude'], np.log(z_min)*100,\n",
    "           c=z_min, cmap=make_cmap('white','green'))\n",
    "plt.ylabel('Latitude')\n",
    "plt.xlabel('Longitude')\n",
    "plt.title('Minimum Daily Flows by Location')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphic Presentation\n",
    "#### Make sure to label all the axis, add legends and units (where appropriate).\n",
    "\n",
    "\n",
    "## Code Quality\n",
    "#### While code performance and optimization won't count, all the code should be highly readable, and reusable. Where possible, create functions, build helper functions where needed, and make sure the code is self-explanatory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
